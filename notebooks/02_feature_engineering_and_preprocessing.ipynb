{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42129d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Raw data loaded successfully.\n",
      "\n",
      "Starting feature engineering...\n",
      "Feature engineering complete. Displaying new features:\n",
      "                                                 url      status  url_length  \\\n",
      "0              http://www.crestonwood.com/router.php  legitimate          37   \n",
      "1  http://shadetreetechnology.com/V4/validation/a...    phishing          77   \n",
      "2  https://support-appleld.com.secureupdate.duila...    phishing         126   \n",
      "3                                 http://rgipt.ac.in  legitimate          18   \n",
      "4  http://www.iracing.com/tracks/gateway-motorspo...  legitimate          55   \n",
      "\n",
      "   hostname_length  path_length  count_hyphens  count_dots  count_at  \\\n",
      "0               19           11              0           3         0   \n",
      "1               23           47              0           1         0   \n",
      "2               50           20              1           4         0   \n",
      "3               11            0              0           2         0   \n",
      "4               15           33              2           2         0   \n",
      "\n",
      "   count_questionmark  count_equals  count_slashes  count_digits  has_ip  \\\n",
      "0                   0             0              3             0       0   \n",
      "1                   0             0              5            17       0   \n",
      "2                   1             3              5            19       0   \n",
      "3                   0             0              2             0       0   \n",
      "4                   0             0              5             0       0   \n",
      "\n",
      "   has_https  is_shortened  \n",
      "0          0             0  \n",
      "1          0             0  \n",
      "2          1             0  \n",
      "3          0             0  \n",
      "4          0             0  \n",
      "\n",
      "Starting data preprocessing...\n",
      "Target variable encoded.\n",
      "{'legitimate': np.int64(0), 'phishing': np.int64(1)}\n",
      "Data split into training and testing sets.\n",
      "X_train shape: (9144, 13)\n",
      "X_test shape: (2286, 13)\n",
      "y_train shape: (9144,)\n",
      "y_test shape: (2286,)\n",
      "\n",
      "Saving processed data...\n",
      "Processed data saved successfully to ../data/processed/\n"
     ]
    }
   ],
   "source": [
    "# AI-Powered Phishing URL Detector\n",
    "# Notebook 2: Feature Engineering and Preprocessing\n",
    "\n",
    "# ## 2.1 Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ## 2.2 Load Raw Data\n",
    "# Load the dataset we prepared in the previous notebook.\n",
    "\n",
    "# Define file paths\n",
    "raw_data_path = '../data/raw/dataset_phishing.csv'\n",
    "processed_data_path = '../data/processed/'\n",
    "os.makedirs(processed_data_path, exist_ok=True) # Ensure the processed data directory exists\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading raw data...\")\n",
    "try:\n",
    "    df = pd.read_csv(raw_data_path)\n",
    "    # We only need the 'url' and 'status' columns for our feature engineering.\n",
    "    # This is a crucial step to ensure we build the logic from scratch.\n",
    "    df = df[['url', 'status']]\n",
    "    print(\"Raw data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {raw_data_path} was not found.\")\n",
    "    df = None\n",
    "\n",
    "# ## 2.3 Feature Engineering\n",
    "# This is where we extract meaningful features from the raw URL strings.\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nStarting feature engineering...\")\n",
    "\n",
    "    # 1. Length-based Features\n",
    "    df['url_length'] = df['url'].apply(len)\n",
    "    df['hostname_length'] = df['url'].apply(lambda x: len(urlparse(x).netloc))\n",
    "    df['path_length'] = df['url'].apply(lambda x: len(urlparse(x).path))\n",
    "\n",
    "    # 2. Count-based Features\n",
    "    def count_char(char, text):\n",
    "        return text.count(char)\n",
    "\n",
    "    df['count_hyphens'] = df['url'].apply(lambda x: count_char('-', x))\n",
    "    df['count_dots'] = df['url'].apply(lambda x: count_char('.', x))\n",
    "    df['count_at'] = df['url'].apply(lambda x: count_char('@', x))\n",
    "    df['count_questionmark'] = df['url'].apply(lambda x: count_char('?', x))\n",
    "    df['count_equals'] = df['url'].apply(lambda x: count_char('=', x))\n",
    "    df['count_slashes'] = df['url'].apply(lambda x: count_char('/', x))\n",
    "    df['count_digits'] = df['url'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "\n",
    "    # 3. Binary Features (Presence of suspicious elements)\n",
    "    def has_ip_address(url):\n",
    "        # Regex to check for an IP address in the hostname\n",
    "        match = re.search(\n",
    "            # IPv4 regex\n",
    "            r'(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'\n",
    "            r'([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|' \n",
    "            # IPv4 in host name\n",
    "            r'((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2}))'\n",
    "            # IPv6 regex\n",
    "            r'([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|'\n",
    "            r'([0-9a-fA-F]{1,4}:){1,7}:|'\n",
    "            r'([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|'\n",
    "            r'([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|'\n",
    "            r'([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|'\n",
    "            r'([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|'\n",
    "            r'([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|'\n",
    "            r'[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|'\n",
    "            r':((:[0-9a-fA-F]{1,4}){1,7}|:)', urlparse(url).netloc)\n",
    "        \n",
    "        return 1 if match else 0\n",
    "\n",
    "    df['has_ip'] = df['url'].apply(has_ip_address)\n",
    "    df['has_https'] = df['url'].apply(lambda x: 1 if urlparse(x).scheme == 'https' else 0)\n",
    "\n",
    "    # Check for common shortening services\n",
    "    shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
    "                          r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|zpr\\.io|tcrn\\.ch|\" \\\n",
    "                          r\"filoops\\.info|v\\.gd|tr\\.im|link\\.zip\\.net\"\n",
    "                          \n",
    "    df['is_shortened'] = df['url'].apply(lambda x: 1 if re.search(shortening_services, x) else 0)\n",
    "\n",
    "    print(\"Feature engineering complete. Displaying new features:\")\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "# ## 2.4 Data Preprocessing\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nStarting data preprocessing...\")\n",
    "    # Define features (X) and target (y)\n",
    "    # We drop the original 'url' column as we can't use text in the model.\n",
    "    X = df.drop(columns=['url', 'status'])\n",
    "    y = df['status']\n",
    "\n",
    "    # Encode the target variable ('phishing' -> 1, 'legitimate' -> 0)\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    print(\"Target variable encoded.\")\n",
    "    # Show mapping\n",
    "    print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    # We use stratify=y_encoded to ensure the class distribution is the same in both sets,\n",
    "    # which is important for our balanced dataset.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    print(\"Data split into training and testing sets.\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "# ## 2.5 Save Processed Data\n",
    "if 'X_train' in locals():\n",
    "    print(\"\\nSaving processed data...\")\n",
    "    X_train.to_csv(os.path.join(processed_data_path, 'X_train.csv'), index=False)\n",
    "    X_test.to_csv(os.path.join(processed_data_path, 'X_test.csv'), index=False)\n",
    "    # Save y_train and y_test as well, as they are numpy arrays now\n",
    "    np.save(os.path.join(processed_data_path, 'y_train.npy'), y_train)\n",
    "    np.save(os.path.join(processed_data_path, 'y_test.npy'), y_test)\n",
    "    print(\"Processed data saved successfully to ../data/processed/\")\n",
    "else:\n",
    "    print(\"\\nPreprocessing was not completed. Skipping save step.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
