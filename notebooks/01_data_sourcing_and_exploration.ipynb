{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Powered Phishing URL Detector\n",
    "# Notebook 1: Data Sourcing and Exploration (Revised)\n",
    "\n",
    "# ## 1.1 Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "\n",
    "# ## 1.2 Data Sourcing (Revised for Raw URLs)\n",
    "# To build a realistic model, we need a dataset of raw URLs, not pre-computed features.\n",
    "# We will use a dataset available on GitHub that contains a simple list of URLs\n",
    "# labeled as 'benign' or 'phishing'.\n",
    "# The code below will download this CSV file and save it to our local project directory.\n",
    "\n",
    "# URL of the raw CSV dataset\n",
    "url = 'https://raw.githubusercontent.com/sahil-gidwani/phishing-detection/master/data/url_data.csv'\n",
    "\n",
    "# Define the path to save the data\n",
    "raw_data_path = '../data/raw/'\n",
    "csv_filename = 'raw_url_dataset.csv'\n",
    "csv_filepath = os.path.join(raw_data_path, csv_filename)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(raw_data_path, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(csv_filepath):\n",
    "    print('Downloading dataset...')\n",
    "    try:\n",
    "        # Use pandas to read the CSV directly from the URL\n",
    "        df = pd.read_csv(url)\n",
    "\n",
    "        # Save the dataframe to our local directory\n",
    "        df.to_csv(csv_filepath, index=False)\n",
    "        print(f'Dataset saved successfully to {csv_filepath}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {e}')\n",
    "else:\n",
    "    print('Dataset already exists. Loading from disk.')\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "\n",
    "\n",
    "# ## 1.3 Initial Data Exploration\n",
    "# Now that we have our raw URL data loaded, let's perform some basic checks.\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Get a summary of the dataframe's structure\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Get descriptive statistics for the columns\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "\n",
    "# ## 1.4 Target Variable Analysis\n",
    "# The 'result' column is our target variable. Let's analyze its distribution.\n",
    "\n",
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='result', data=df, palette=['#32cd32', '#ff6347'], order=['benign', 'phishing'])\n",
    "plt.title('Distribution of Phishing vs. Benign URLs', fontsize=16)\n",
    "plt.xlabel('URL Type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "class_counts = df['result'].value_counts()\n",
    "class_percentages = df['result'].value_counts(normalize=True) * 100\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(class_counts)\n",
    "print(\"\\nClass Percentages:\")\n",
    "print(class_percentages)\n",
    "\n",
    "\n",
    "# ## 1.5 Initial Findings & Next Steps\n",
    "#\n",
    "# Based on our revised exploration:\n",
    "#\n",
    "# 1.  **Dataset Shape:** The new dataset contains **450,176 samples** and 2 columns (`url` and `result`). This is a much larger and more robust dataset.\n",
    "# 2.  **Data Content:** We have exactly what we need: the raw `url` as a string and the `result` label ('benign' or 'phishing').\n",
    "# 3.  **No Missing Values:** The dataset is clean, with no missing URLs or labels.\n",
    "# 4.  **Target Distribution:** The classes are not perfectly balanced. We have significantly more benign URLs (~77%) than phishing URLs (~23%). This is a realistic scenario. We must keep this imbalance in mind during model evaluation, where metrics like Precision and Recall will be more important than simple Accuracy.\n",
    "#\n",
    "# **Next Step:** We are now perfectly positioned to move on to **Notebook 02**. In the next stage, we will take the raw strings from the `url` column and perform **feature engineering** to create the numerical inputs our machine learning models will need.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
